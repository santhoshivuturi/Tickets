Datasets:
case class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String, count: BigInt)
	-- Here we are defining a case class.
val flightsDF = spark.read.parquet("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/Data/flight-data/parquet/2010-summary.parquet/")	
	-- We will read a file and that is still a dataframe but we can need to change it like we want.
flightsDF.printSchema()
root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORIGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)
 	-- the above is the way which spark is reading a Dataframe on it own by taking its own types and we defined our owm in case class and we are gonns use that to chnage the types in the Dataframe that we have created	
val flights = flightsDF.as[Flight]
	-- Now we are changing the Dataframe which we have created by loading which is flightsDF to case class flight 
	-- when we see in the above schema spark has considered the above count as long but we have chnaged that to bigInt by using case class
flights.show(2)
	-- Actions we use are same for dataframes and datasets
flights.first.DEST_COUNTRY_NAME	output:res47: String = United States 
	--we donâ€™t need to do any type coercion, we simply specify the named attribute of the case class and get back, not just the expected value but the expected type, as well:
Transformations for Datasets:
	-- Transformation for Datsets are same as those of which we saw for dataframes.Any transformation that we would read in this section is valid on a dataset , and we encourage you to look through the specific sections on relevant aggregations or joins.
	-- In addition to those transformation datasets alsi allow us to specify more complex and strongly typed transformations that we could perform of Dataframes alone because we manipulate Raw JVM types. 
	-- To get grip of the raw object manipulation we will be doing a filter operation on the data type we created.
Filtering:
	-- We will be doing a simple example in which we will be creating a simple function that accepts a flight and simply returns a boolean value that described the origin of the flight and destination are same This is not a UDF but a generic function.
	-- Now we will be defining a function to achive the filter which we want or the above which we are trying to acheive.
	-- by specifiying the function we are asking or forcing spark to evaluate this function on every row in the dataset.This can be very resource intensive for a simple filter operation it is always preferred to write a SQL expression. This will greatly help us to reducing the cost of filtering out the data and also allowing us to manipulate the dataframe which we have filter to a dataset later on 
def originIsDestination(flight_row: Flight): Boolean = {
return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME
}	
	-- Here we are defining a function to apply the filter which we want in which the filghts destination country name and orgin country name are same and should return a boolean value to check that.
	-- Now we can pass this function to the filter method specifying the each row it should verify that this function return true and this process will filter out the data accordingly.
flights.filter(flight_row => originIsDestination(flight_row)).first()
	-- here we using the dataset we have created and appliying a filter and saying that use this function in the filter and we said first so it shows the first one.
output:
Flight = Flight(United States,United States,348113)
	-- It shows the matching and return a booelan value as defined.
	-- As we saw earlier, this function does not need to execute in Spark code at all. Similar to our UDFs, we can use it and test it on data on our local machines before using it within Spark.
flights.collect().filter(flight_row => originIsDestination(flight_row))
	-- As this dataset is small enough to collect to a driver on which we can perform same collect operation and filter as well.
output:
	Array[Flight] = Array(Flight(United States,United States,348113))
Mapping:
	-- Filtering is a simple operation but sometimes we need to map one value to another. We did this with our function in previous example: it accepts a filgth and return a boolean value but other times we might actually need to perform something more sophisticates as Extract value compare a set of values or something similar 
	-- The simplest example n maipulation our Dataset such that we can extract on value from each row This effectively performing the dataframe like select on our dataset.
	-- Extracting the destination.
val destinations = flights.map(f => f.DEST_COUNTRY_NAME)
	-- when we do the above we end up with a Dataset of type string. This is because spark already knows the JVM type that this sult should return and allows us to benfit from compile time checking if for some reason it is invalid.We can collect this and get back an array of strings on the driver:
val localDestinations = destinations.take(5)
	-- now we are taking the 5 from the destination that we have extracted from the entire dataset and we all know this is not possible on Dataframes.
	if we do so we can achieve the same with dataframes and it is also recommended to do that but if you want to use this case then there should be something related to column by column maniplutaion to do so.
Joins:
	-- they are similar to that we have done with dataframes also datasets provide more sophisticates way that is joinWith methods. joinWith is almost similar to co-group with on RDD's and we basically end up with 2 nested datasets in "1".
	-- Each column represents on Dataset and they can be manipulated accordingly.
	-- This can be useful when we need to maintain more information in the join or perform somemore sophisticated anipulation on the entire result set like an advance map or filter.
	-- Creating a fake flight metadata dataset to demonstrate joinWith:					
case class FlightMetadata(count: BigInt, randomData: BigInt)
	-- function and we are defining that with a case class and we are saying count is bigInt and randomData bigInt.
val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong)).withColumnRenamed("_1", "count").withColumnRenamed("_2", "randomData").as[FlightMetadata]	
	-- here we are using spark range function and mentioning the range as 500 and saying .map x=> x , random long and with column renames as _1 as count and _2 as randomData
val flights2 = flights.joinWith(flightsMeta, flights.col("count") === flightsMeta.col("count"))	
	-- here we are using joining flights with flightsmeta where flights.count column is equal to flightsMeta.coount column.
	-- when we do this we end up with a dataset of type key value pairs in which each row represents flight and flight metadata we can query this as dataset or dataframe of type complex
flights2.selectExpr("_1.DEST_COUNTRY_NAME")
	-- We are acessing it as dataframe and as complex type too.
flights2.take(2)
	-- We can collect them as we did before.
val flights2 = flights.join(flightsMeta, Seq("count"))
	-- normal join is alos possible but we will be ending up loosing the type data in JVM