Oozie:
-- Spark can also be integrated with Oozie.
-- Comes by default in hadoop 
-- work flow and co ordination.
-- direct acyclic graph 
-- We mainly interact with command line and there will always be a tomcat instance running.
-- We can see all the action submitted in oozie UI.
-- Runs on hhtp service thats the reason we use tomcat server.
-- Workflows are defined via a XML file.
-- oozie have action nodes and contol nodes.
-- action depends on what action we are doing.

Control Flow:
-- Start End Kill
-- Decision
-- fork Join

Actions:
-- Mapreduce
-- java
-- pig
-- hdfs
-- hive 
-- shell

Inside a workflow we define what does what and what should be performed first and all.

Oozie Coordination engine:
-- its based on the time we give.
-- file based scheduling.


oozie admin -oozie http://nn01.itversity.com:11000/oozie
oozie-examples.tar.gz
/usr/hdp/2.5.0.0-1245/oozie/doc/oozie-examples.tar.gz
/home/santhoshi/exampleoozie/
job.properties:
---------------
nameNode=hdfs://nn01.itversity.com:8020/user/
jobtracker=rm01.itversity.com:8050
queue_Name=default
examplesRoot=examples

oozie.wf.application.path=hdfs://nn01.itversity.com:8020/user/santhoshi/oozie/workflows/wf.xml




<workflow-app xmlns="uri:oozie:workflow:0.2" name="retailers">
    <start to="hiveexamplesone-node"/>

    <action name="hive-node">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data/hive"/>
                <mkdir path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <script>script.q</script>
            <param>INPUT=/user/${wf:user()}/${examplesRoot}/input-data/table</param>
            <param>OUTPUT=/user/${wf:user()}/${examplesRoot}/output-data/hive</param>
        </hive>
        <ok to="end"/>
        <error to="fail"/>
    </action>

    <kill name="fail">
        <message>Hive failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>

